{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data files\n",
    "# https://www.irs.gov/statistics/soi-tax-stats-migration-data\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "requests_cache.install_cache()\n",
    "\n",
    "result = requests.get('https://www.irs.gov/statistics/soi-tax-stats-migration-data')\n",
    "assert result.status_code == 200\n",
    "c = result.content\n",
    "\n",
    "soup = BeautifulSoup(c)\n",
    "\n",
    "my_target = lambda tag: tag.name == 'h3' and 'State-to-State Migration Data' in tag.get_text()\n",
    "\n",
    "# get 1990 to 2011 data urls\n",
    "data_links = soup.find(my_target).next_sibling.next_sibling.find_all('a')\n",
    "\n",
    "urls = [{'url': a['href'], 'date_range': a.string} for a in data_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "class LiveServerSession(Session):\n",
    "    def __init__(self, prefix_url=None, *args, **kwargs):\n",
    "        super(LiveServerSession, self).__init__(*args, **kwargs)\n",
    "        self.prefix_url = prefix_url\n",
    "\n",
    "    def request(self, method, url, *args, **kwargs):\n",
    "        url = urljoin(self.prefix_url, url)\n",
    "        return super(LiveServerSession, self).request(method, url, *args, **kwargs)\n",
    "    \n",
    "def get_soi_data(url):\n",
    "    baseUrl = 'https://www.irs.gov'\n",
    "        \n",
    "    try:\n",
    "        with LiveServerSession(baseUrl) as s:\n",
    "            r = s.get(url)\n",
    "        assert r.status_code == 200\n",
    "    except:\n",
    "        raise\n",
    "        \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soi_details(url):\n",
    "    '''\n",
    "    get cumulative data as df given lookup and year\n",
    "    '''\n",
    "    from io import BytesIO\n",
    "    from zipfile import ZipFile\n",
    "    import itertools\n",
    "    \n",
    "    r = get_soi_data(url)\n",
    "    \n",
    "    pd_options = {\n",
    "        'header': None\n",
    "    }\n",
    "    \n",
    "    if '.zip' in r.url:\n",
    "        with ZipFile(BytesIO(r.content)) as my_zipfile:\n",
    "            my_details = {}\n",
    "            \n",
    "            # filter down to Illinois data\n",
    "            zip_list = list(filter(lambda x: re.match('(.*/)?il|.*il\\wr|.*mig(in|out)il', x, re.IGNORECASE), my_zipfile.namelist()))\n",
    "            \n",
    "            for file in zip_list:\n",
    "                chunk = file.split('/')[-1]\n",
    "                \n",
    "                if re.search('ou?(t|r)', chunk, flags=re.IGNORECASE):\n",
    "                    my_details['file_out'] = file\n",
    "                else:\n",
    "                    my_details['file_in'] = file\n",
    "                    \n",
    "            return my_details\n",
    "\n",
    "for entry in urls:\n",
    "    entry.update(get_soi_details(entry['url']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip writing original/il_in_1990_1991.xls because file already exists!\n",
      "Skip writing original/il_in_1991_1992.xls because file already exists!\n",
      "Skip writing original/il_in_1992_1993.xls because file already exists!\n",
      "Skip writing original/il_in_1993_1994.xls because file already exists!\n",
      "Skip writing original/il_in_1994_1995.xls because file already exists!\n",
      "Skip writing original/il_in_1995_1996.xls because file already exists!\n",
      "Skip writing original/il_in_1996_1997.xls because file already exists!\n",
      "Skip writing original/il_in_1997_1998.xls because file already exists!\n",
      "Skip writing original/il_in_1998_1999.xls because file already exists!\n",
      "Skip writing original/il_in_1999_2000.xls because file already exists!\n",
      "Skip writing original/il_in_2000_2001.xls because file already exists!\n",
      "Skip writing original/il_in_2001_2002.xls because file already exists!\n",
      "Skip writing original/il_in_2002_2003.xls because file already exists!\n",
      "Skip writing original/il_in_2003_2004.xls because file already exists!\n",
      "Skip writing original/il_in_2004_2005.xls because file already exists!\n",
      "Skip writing original/il_in_2005_2006.xls because file already exists!\n",
      "Skip writing original/il_in_2006_2007.xls because file already exists!\n",
      "Skip writing original/il_in_2007_2008.xls because file already exists!\n",
      "Skip writing original/il_in_2008_2009.xls because file already exists!\n",
      "Skip writing original/il_in_2009_2010.xls because file already exists!\n",
      "Skip writing original/il_in_2010_2011.xls because file already exists!\n",
      "Skip writing original/il_out_1990_1991.xls because file already exists!\n",
      "Skip writing original/il_out_1991_1992.xls because file already exists!\n",
      "Skip writing original/il_out_1992_1993.xls because file already exists!\n",
      "Skip writing original/il_out_1993_1994.xls because file already exists!\n",
      "Skip writing original/il_out_1994_1995.xls because file already exists!\n",
      "Skip writing original/il_out_1995_1996.xls because file already exists!\n",
      "Skip writing original/il_out_1996_1997.xls because file already exists!\n",
      "Skip writing original/il_out_1997_1998.xls because file already exists!\n",
      "Skip writing original/il_out_1998_1999.xls because file already exists!\n",
      "Skip writing original/il_out_1999_2000.xls because file already exists!\n",
      "Skip writing original/il_out_2000_2001.xls because file already exists!\n",
      "Skip writing original/il_out_2001_2002.xls because file already exists!\n",
      "Skip writing original/il_out_2002_2003.xls because file already exists!\n",
      "Skip writing original/il_out_2003_2004.xls because file already exists!\n",
      "Skip writing original/il_out_2004_2005.xls because file already exists!\n",
      "Skip writing original/il_out_2005_2006.xls because file already exists!\n",
      "Skip writing original/il_out_2006_2007.xls because file already exists!\n",
      "Skip writing original/il_out_2007_2008.xls because file already exists!\n",
      "Skip writing original/il_out_2008_2009.xls because file already exists!\n",
      "Skip writing original/il_out_2009_2010.xls because file already exists!\n",
      "Skip writing original/il_out_2010_2011.xls because file already exists!\n"
     ]
    }
   ],
   "source": [
    "def save_soi_file(url, file, date_range, direction):\n",
    "    from io import BytesIO\n",
    "    from zipfile import ZipFile\n",
    "    import itertools\n",
    "    \n",
    "    r = get_soi_data(url)\n",
    "    \n",
    "    pd_options = {\n",
    "        'header': None\n",
    "    }\n",
    "    \n",
    "    with ZipFile(BytesIO(r.content)) as zf:        \n",
    "        target_path = 'original/il_{}_{}_{}.{}'.format(\n",
    "            direction,\n",
    "            date_range.split(' to ')[0],\n",
    "            date_range.split(' to ')[1],\n",
    "            file.split('.')[-1]\n",
    "        )\n",
    "        try:\n",
    "            with open(target_path, 'wb') as f:\n",
    "                f.write(zf.read(file))\n",
    "        except:\n",
    "            print(f'Skip writing {target_path} because file already exists!')\n",
    "        \n",
    "\n",
    "entry = urls[0]\n",
    "\n",
    "for direction in ('in', 'out'):\n",
    "    for entry in urls:\n",
    "        save_soi_file(entry['url'], entry[f'file_{direction}'], entry['date_range'], direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df):\n",
    "    \n",
    "    df = df[df.columns[-6:]]\n",
    "    df.columns = [\n",
    "        \"Origin from\",\n",
    "        \"State\",\n",
    "        \"State Name\",\n",
    "        \"Number of returns\",\n",
    "        \"Number of exemptions\",\n",
    "        \"Aggregate adjusted gross income (AGI)\",\n",
    "    ]\n",
    "\n",
    "    def test_returns(x):\n",
    "        try:\n",
    "            return int(x)\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    df = df[df['Number of returns'].apply(test_returns) > 0]\n",
    "    \n",
    "    return df\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for direction in ('in', 'out'):\n",
    "    for x in range(2010, 1989, -1):\n",
    "        filename = f'il_{direction}_{x}_{x+1}.xls'\n",
    "        df = pd.read_excel(f'working/{filename}')\n",
    "        df = transform_df(df)\n",
    "\n",
    "        state, direction, date_from, date_to = filename.split('.')[0].split('_')\n",
    "        df['direction'] = direction\n",
    "        df['date_from'] = date_from\n",
    "        df['date_to'] = date_to\n",
    "\n",
    "        dfs.append(df)\n",
    "    \n",
    "big_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df['Origin from'] = big_df['Origin from'].apply(lambda x: str(x).zfill(2))\n",
    "big_df['State'] = big_df['State'].str.upper()\n",
    "big_df['State Name'] = big_df['State Name'].str.title().str.replace(' Of ', ' of ')\n",
    "\n",
    "num_cols = ['Number of returns', 'Number of exemptions', 'date_to', 'date_from']\n",
    "big_df[num_cols] = big_df[num_cols].astype(int)\n",
    "\n",
    "my_df = big_df.set_index(['direction', 'Origin from', 'date_to']).sort_index()\n",
    "\n",
    "# for key in my_df.index.get_level_values('Origin from').unique():\n",
    "#     this_df = my_df.loc[my_df.index.get_level_values('Origin from') == key]\n",
    "#     display(this_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_df = my_df.query('date_to == 1991 and direction == \"in\"')\n",
    "\n",
    "# this_df.loc[~this_df.index.get_level_values('Origin from').isin(['17', '96'])].iloc[:, 2:5].sum().values\n",
    "\n",
    "# this_df.loc[this_df.index.get_level_values('Origin from') == '96'].iloc[:, 2:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_df = my_df.copy()\n",
    "this_df = this_df.loc[this_df.index.get_level_values('Origin from') == '96']\n",
    "\n",
    "pivot = this_df.reset_index().pivot(index='date_to', columns='direction', values='Number of returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>direction</th>\n",
       "      <th>in</th>\n",
       "      <th>out</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_to</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>97133</td>\n",
       "      <td>114303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>97177</td>\n",
       "      <td>113789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>96732</td>\n",
       "      <td>114691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>93362</td>\n",
       "      <td>115103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>98259</td>\n",
       "      <td>118129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>97167</td>\n",
       "      <td>116823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>97206</td>\n",
       "      <td>122718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>98765</td>\n",
       "      <td>124806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>102659</td>\n",
       "      <td>125262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>102905</td>\n",
       "      <td>128306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>104233</td>\n",
       "      <td>128424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>100309</td>\n",
       "      <td>126190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>96146</td>\n",
       "      <td>121006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>95941</td>\n",
       "      <td>121763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>99470</td>\n",
       "      <td>122661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>108029</td>\n",
       "      <td>124089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>110585</td>\n",
       "      <td>119762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>112836</td>\n",
       "      <td>122724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>109671</td>\n",
       "      <td>118378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>90489</td>\n",
       "      <td>112636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>93055</td>\n",
       "      <td>119487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "direction      in     out\n",
       "date_to                  \n",
       "1991        97133  114303\n",
       "1992        97177  113789\n",
       "1993        96732  114691\n",
       "1994        93362  115103\n",
       "1995        98259  118129\n",
       "1996        97167  116823\n",
       "1997        97206  122718\n",
       "1998        98765  124806\n",
       "1999       102659  125262\n",
       "2000       102905  128306\n",
       "2001       104233  128424\n",
       "2002       100309  126190\n",
       "2003        96146  121006\n",
       "2004        95941  121763\n",
       "2005        99470  122661\n",
       "2006       108029  124089\n",
       "2007       110585  119762\n",
       "2008       112836  122724\n",
       "2009       109671  118378\n",
       "2010        90489  112636\n",
       "2011        93055  119487"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name('/Users/pjudge/.credentials/BGA Graphics-3edf4552f3a5.json', scope)\n",
    "\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "\n",
    "# worksheet = gc.open_by_key('1bUClFkz2bTx9moNSy6fiXmpVQZPeXGYQPb598iix8TA').worksheet('data')\n",
    "worksheet = gc.open_by_key('1Y1jrpI2hqB1wK3taXPC3TCuyhWmZmWkFq1gEqh3e2MI').worksheet('data')\n",
    "\n",
    "def blank_out_worksheet(worksheet):\n",
    "    \"\"\"\n",
    "    totally blank out worksheet\n",
    "    \"\"\"\n",
    "    from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "    \n",
    "    zeroed_df = get_as_dataframe(worksheet)\n",
    "    \n",
    "    # set vals to null\n",
    "    zeroed_df[:] = np.nan\n",
    "    \n",
    "    # set cols to null\n",
    "    zeroed_df.rename(columns=lambda x: np.nan, inplace=True)\n",
    "    \n",
    "    # set worksheet to blank dataframe\n",
    "    set_with_dataframe(worksheet, zeroed_df)\n",
    "    \n",
    "df_out = pivot.copy()\n",
    "df_out.out = df_out.out.apply(lambda x: -x)\n",
    "df_out = df_out.astype(int).reset_index()\n",
    "\n",
    "df_out.columns = ['label', 'inflow', 'outflow']\n",
    "\n",
    "blank_out_worksheet(worksheet)\n",
    "set_with_dataframe(worksheet, df_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
